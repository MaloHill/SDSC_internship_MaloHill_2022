{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f126bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tk\n",
    "import scipy.stats as st\n",
    "import matplotlib.colors as cl\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "### Fix directories\n",
    "\n",
    "data_dir = \"../data/initial database/output\" #path to project folder\n",
    "\n",
    "os.chdir(data_dir) #Select the project directory\n",
    "\n",
    "### Import the data from the csv files\n",
    "\n",
    "def select_directory(sample, file_name = \"compounds\", part = 1):\n",
    "    \"\"\"Returns the file directory corresponding to the file_name.csv file in the folder corresponding to the given sample\n",
    "    (DiAcids, Fallopia, Ruthenium or Ruthenium2).\n",
    "    \n",
    "    the input part indicates the subfolder in which it is stored (pt1 or pt2).\"\"\"\n",
    "    return data_dir + \"/\" + sample + \"/pt\" + str(part) + \"/\" + file_name + \".csv\"\n",
    "\n",
    "def df_from_csv(directory):\n",
    "    \"\"\"Extracts the csv file of the file contained in the directory and stores it in a pandas dataframe.\"\"\"\n",
    "    with open(directory) as file:\n",
    "        return pd.read_csv(file)\n",
    "\n",
    "#Convert the data into pandas dataframes and compute intratios.\n",
    "\n",
    "sample_names = [\"DiAcids\", \"Fallopia\", \"Ruthenium\", \"Ruthenium2\"]\n",
    "\n",
    "data = pd.DataFrame({\"compounds\" : [df_from_csv(select_directory(sample)) for sample in sample_names],\n",
    "                     \"peaks\" : [df_from_csv(select_directory(sample, file_name = \"ms2_peaks\")) for sample in sample_names],\n",
    "                     \"spectra\" : [df_from_csv(select_directory(sample, file_name = \"ms2_spectra\")) for sample in sample_names]\n",
    "                    }, index = sample_names)\n",
    "\n",
    "def peaks_in_list(sample):\n",
    "    \"\"\"Returns a dataframe indexed by spectrum_id with entries lists of mz, list of\n",
    "    intensities and sum of intensities of the ms2 peaks corresponding to this spectrum id.\"\"\"\n",
    "    \n",
    "    groups = data[\"peaks\"][sample].groupby([\"spectrum_id\"])\n",
    "    df = groups.agg(list)\n",
    "    \n",
    "    return df.loc[:,[\"mz\", \"intensity\"]]\n",
    "\n",
    "list_format_sp = pd.Series([peaks_in_list(sample) for sample in sample_names], index = sample_names)\n",
    "\n",
    "def get_spectrum(sample, spectrum_id):\n",
    "    \"\"\"Select a spectrum in the list_format_sp dataframe.\"\"\"\n",
    "    return pd.Series(list_format_sp[sample].intensity[spectrum_id], index = list_format_sp[sample].mz[spectrum_id])\n",
    "\n",
    "def display(spectrum):\n",
    "    x = spectrum.index\n",
    "    plt.bar(x, spectrum, width = .5)\n",
    "    plt.yscale(\"log\")\n",
    "    \n",
    "def make_table(sample, spectrum_id):\n",
    "    \"\"\"Make a contingency table out of the two spectra sp1 and sp2. Only consider peaks that are in both spectra.\n",
    "    \n",
    "    Input should be in the following form\n",
    "    \n",
    "    sample : a couple of sample names (\"Fallopia\", \"DiAcids\", \"Ruthenium\" or \"Ruthenium2\") ;\n",
    "    spectrum_id : a couple of spectrum_id of the corresponding sample.\n",
    "    \n",
    "    The output is a 2 * d contingency tables containing the intensities of the selected peaks, where d is the number of selected peaks in each spectrum.    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Extract the spectra as table with two columns, mz and intensity.\n",
    "    sp1 = pd.DataFrame({\"mz\" : list_format_sp[sample[0]][\"mz\"][spectrum_id[0]],\n",
    "                        \"intensity\" : list_format_sp[sample[0]][\"intensity\"][spectrum_id[0]]\n",
    "                       })\n",
    "    \n",
    "    sp2 = pd.DataFrame({\"mz\" : list_format_sp[sample[1]][\"mz\"][spectrum_id[1]],\n",
    "                        \"intensity\" : list_format_sp[sample[1]][\"intensity\"][spectrum_id[1]]\n",
    "                       })\n",
    "    \n",
    "    #Initialization of the rows of the output table.\n",
    "    row1, row2 = [], []\n",
    "    \n",
    "    k1, k2 = len(sp1), len(sp2)\n",
    "    \n",
    "    #Indices running through the mz list.\n",
    "    i, j = 0, 0\n",
    "    while i < k1 and j < k2:\n",
    "        \n",
    "        mz1, mz2 = sp1.mz[i], sp2.mz[j]\n",
    "        if abs(mz1 - mz2) < 1e-5 * (mz1 + mz2) / 2: #Machine precision is 10ppm.\n",
    "            #if the peaks have similar mz, their intensities are stored in row1 and row2.\n",
    "            row1.append(sp1.intensity[i])\n",
    "            row2.append(sp2.intensity[j])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        \n",
    "        #If the mz are distinct, the index corresponding to the smallest mz moves on to the next one (the mz are sorted in incresasing order in the sp_i.mz lists)\n",
    "        elif sp1.mz[i] < sp2.mz[j]:\n",
    "            i += 1\n",
    "            \n",
    "        else :\n",
    "            j += 1\n",
    "            \n",
    "    return np.array([row1, row2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05364585",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "#Rename the dataframe to work with.\n",
    "peaks = data[\"peaks\"][\"Fallopia\"]\n",
    "spectra = data[\"spectra\"][\"Fallopia\"]\n",
    "compounds = data[\"compounds\"][\"Fallopia\"]\n",
    "\n",
    "\n",
    "compounds.index = compounds[\"compound_id\"]\n",
    "\n",
    "spid_count_by_cid = spectra.groupby(\"compound_id\")[\"spectrum_id\"].count()\n",
    "all_cid = spid_count_by_cid[spid_count_by_cid > 1].index\n",
    "nb_cid = len(all_cid) #This is 794. There are 841 compound ids but only 794 have at least two spectra.\n",
    "\n",
    "#First separate the compounds into a training set of size 320 and an evaluating set of size 80.\n",
    "#This needs to be done so that there is enough pairs of compound id with the same molecular weight in the evaluation set.\n",
    "\n",
    "#Create a data frame with columns compound id and parentmz rounded to the third decimal, sorted by parentmz.\n",
    "spectra_by_cid = pd.DataFrame((1000 * spectra.groupby(\"compound_id\")[\"parent_mz\"].first().sort_values()).apply(int))\n",
    "spectra_by_cid[\"compound_id\"] = spectra_by_cid.index\n",
    "\n",
    "#Aggregate the compound id with same parent mz in lists and select the lists of length > 1.\n",
    "cid_by_pmz = spectra_by_cid.groupby(\"parent_mz\")[\"compound_id\"].agg(list)\n",
    "comparable_cid = cid_by_pmz[np.array([len(x) for x in cid_by_pmz]) > 1]\n",
    "\n",
    "len_comparable = len(comparable_cid)\n",
    "comparable_cid.index = range(len_comparable)\n",
    "\n",
    "#Select at random some of those list, amounting to around 280 cid (~ 400/794 of the comparable compounds id).\n",
    "nb_of_lists = 50\n",
    "nb_of_compounds = 0\n",
    "while nb_of_compounds < 279:\n",
    "    seed = 3177606399\n",
    "    np.random.seed(seed)\n",
    "    lst_of_indexes = np.random.choice(len_comparable, nb_of_lists, replace = False)\n",
    "    nb_of_compounds = sum([len(x) for x in comparable_cid[lst_of_indexes]])\n",
    "    nb_of_lists += 1\n",
    "\n",
    "#From those ~280 cid, select at random ~ 56 of them\n",
    "nb_of_lists = 10\n",
    "nb_of_compounds = 0\n",
    "while nb_of_compounds < 55:\n",
    "    nb_of_lists += 1\n",
    "    lst_of_eval_indexes = lst_of_indexes[:nb_of_lists] #lst_of_indexes is already randomly shuffled.\n",
    "    nb_of_compounds = sum([len(x) for x in comparable_cid[lst_of_eval_indexes]])\n",
    "    \n",
    "eval_cid_flat = []\n",
    "for i in lst_of_eval_indexes:\n",
    "    eval_cid_flat += comparable_cid[i] #Evaluating members of the comparable cid.\n",
    "\n",
    "training_cid_flat = []\n",
    "for i in lst_of_indexes[nb_of_lists:]:\n",
    "    training_cid_flat += comparable_cid[i] #Training members of the comparable cid.\n",
    "\n",
    "\n",
    "#Now, add non-comparable indexes in both the training and the evaluating sets, to a total set of 400 compound ids\n",
    "#Divided into 80 % for training and 20 % for evaluating, with fraction of comparable cid in each subset roughly equal to the\n",
    "#fraction of comparable cid in the total set.\n",
    "\n",
    "comparable_cid_flat = []\n",
    "for lst in comparable_cid:\n",
    "    comparable_cid_flat += lst\n",
    "\n",
    "not_comparable_cid = np.array(all_cid)[np.logical_not([x in comparable_cid_flat for x in all_cid])]\n",
    "\n",
    "\n",
    "seed = 2312570194\n",
    "np.random.seed(seed)\n",
    "random_not_comparable_cid = list(np.random.choice(not_comparable_cid,\n",
    "                                                  400 - len(training_cid_flat) - len(eval_cid_flat),\n",
    "                                                  replace = False\n",
    "                                                 ))\n",
    "missing_in_eval = 80 - len(eval_cid_flat)\n",
    "eval_cid_flat += random_not_comparable_cid[:missing_in_eval] #Complete the evaluation set to 80 compound ids.\n",
    "training_cid_flat += random_not_comparable_cid[missing_in_eval:] #Complete the training set to 320.\n",
    "\n",
    "#Recap :\n",
    "#eval_cid_flat is the list of compound ids in the evaluating set\n",
    "#training_cid_flat is the list of compound ids in the training set\n",
    "\n",
    "#comparable_cid is a series containing lists of cid having the same molecular weight.\n",
    "#lst_of_eval_indexes is the list of indexes of comparables cid corresponding of the evaluating set for power testing\n",
    "#Note : power testing is basically evaluating type 2 error, ie the proportion of false negative. Evaluating it can only be done\n",
    "#on data labelled as negative, which here is spectra from different molecules having the same molecular weight. This is why\n",
    "#preparing an evaluating set containing a fair amount of pairs of comparable compound ids is important (comparable means having\n",
    "#the same molecular weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d91aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_directory = \"../data/experiments/experiment 1/\"\n",
    "\n",
    "\n",
    "#Getting the tables back from the csv files.\n",
    "training = pd.read_csv(folder_directory + \"training_info.csv\")\n",
    "eval_alpha = pd.read_csv(folder_directory + \"eval_alpha_info.csv\")\n",
    "eval_beta = pd.read_csv(folder_directory + \"eval_beta_info.csv\")\n",
    "\n",
    "df = pd.read_csv(folder_directory + \"training_tables.csv\")\n",
    "flat_tables = df.groupby(\"tb_index\")[\"intensity\"].agg(list).apply(np.array)\n",
    "tables = [tb.reshape(2, len(tb) // 2) for tb in flat_tables]\n",
    "nb_col = [len(tb[0]) for tb in tables]\n",
    "\n",
    "training[\"tables\"] = tables\n",
    "training[\"nb_col\"] = nb_col\n",
    "\n",
    "df = pd.read_csv(folder_directory + \"eval_alpha.csv\")\n",
    "flat_tables = df.groupby(\"tb_index\")[\"intensity\"].agg(list).apply(np.array)\n",
    "tables = [tb.reshape(2, len(tb) // 2) for tb in flat_tables]\n",
    "nb_col = [len(tb[0]) for tb in tables]\n",
    "\n",
    "eval_alpha[\"tables\"] = tables\n",
    "eval_alpha[\"nb_col\"] = nb_col\n",
    "\n",
    "df = pd.read_csv(folder_directory + \"eval_beta.csv\")\n",
    "flat_tables = df.groupby(\"tb_index\")[\"intensity\"].agg(list).apply(np.array)\n",
    "tables = [tb.reshape(2, len(tb) // 2) for tb in flat_tables]\n",
    "nb_col = [len(tb[0]) for tb in tables]\n",
    "\n",
    "eval_beta[\"tables\"] = tables\n",
    "eval_beta[\"nb_col\"] = nb_col\n",
    "\n",
    "training_beta = pd.read_csv(folder_directory + \"training_beta_info.csv\")\n",
    "\n",
    "df = pd.read_csv(folder_directory + \"training_tables_beta.csv\")\n",
    "flat_tables = df.groupby(\"tb_index\")[\"intensity\"].agg(list).apply(np.array)\n",
    "tables = [tb.reshape(2, len(tb) // 2) for tb in flat_tables]\n",
    "nb_col = [len(tb[0]) for tb in tables]\n",
    "\n",
    "training_beta[\"tables\"] = tables\n",
    "training_beta[\"nb_col\"] = nb_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e697c",
   "metadata": {},
   "source": [
    "The first goal is to estimate phi1\n",
    "\n",
    "Taking the training tables beta, for each group of compound id (i.e. an element of comparable cid not flat which has been\n",
    "selected for training), select one table by spectrum id with this compound id and the right number of columns and read the\n",
    "value....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10955813",
   "metadata": {},
   "source": [
    "I didn't have the time to make a code for this during the internship, but I made a plan.\n",
    "\n",
    "First : estimate $\\varphi_1$. In order to do this, compute for each family of molecules an estimation of the covariance matrix of the parameters of the prior Dirichlet distribution.\n",
    "\n",
    "Let's fix a number of columns $d$.\n",
    "\n",
    "If compounds 1 to $K$ have the same molecular weight, then the spectra of compound $k$ with $d$ columns provide estimators of the numbers $p_{k, 1}, \\dots, p_{k, d}$ drawed under the prior Dirichlet distributaion and associated to the compound $k$.\n",
    "\n",
    "A moments estimator for the simplex parameter of the prior Dirichlet distribution is given by\n",
    "\n",
    "$$\\hat{\\theta}_i = \\frac{1}{K} \\sum_{k = 1}^K p_{k, i} \\, ;$$\n",
    "\n",
    "and the coefficients of the covariance matrix can then be estimated without bias by\n",
    "\n",
    "$$\\hat{c}_{i, j} = \\frac{1}{K-1} \\sum_{k = 1}^K (p_{k, i} - \\hat{\\theta}_{i}) (p_{k, j} - \\hat{\\theta}_{j}) \\, .$$\n",
    "\n",
    "Under the Dirichlet model, the covariance matrix is given by\n",
    "\n",
    "$$c_{i, j} = (\\delta_{i, j} \\theta_i - \\theta_i \\theta_j) \\frac{1}{1 + \\varphi_1^{-1}} \\, .$$\n",
    "\n",
    "By doing a linear regression, the coefficient $\\frac{1}{1 + \\varphi_1^{-1}}$ can be estimated, and this gives a method to estimate $\\varphi_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb114f",
   "metadata": {},
   "source": [
    "Now we would like to estimate $\\varphi_0$ I suggest to do it in the same way in the training tables (for alpha). This time, this is the spectra which are indexed by $k$ and the covariance is that of a Dirichlet-multinomial distribution instead. It could be envisoined to use exactly the same method by saying that the normalized intensities $X_i / N$ follow a Dirichlet distribution of parameter $\\varphi_0 \\mathbf{p}$ (which is almost true, the error being introduced by the multinomial distribution which has low variance for large N).\n",
    "\n",
    "Now that we have estimated $\\varphi_1$ and $\\varphi_0$, it only remains to compute the test statistic, which expresses as\n",
    "\n",
    "$$T = \\frac{1}{B(\\varphi_1^{-1} \\theta)} \\left( \\int_{\\Delta} \\prod_{j = 1}^d \\frac{p_j^{\\varphi_1^{-1} \\theta_j - 1}}{B(n_{1, j}, \\varphi_0^{-1}p_j}\\mathrm{d}\\, p \\right) \\left( \\int_{\\Delta} \\prod_{j=1}^d \\frac{p_j^{\\varphi_1^{-1} \\theta_j - 1}}{B(n_{2, j}, \\varphi_0^{-1}p_j)} \\mathrm{d} \\, p \\right) \\left(\\int_{\\Delta} \\prod_{j=1}^d \\frac{p_j^{\\varphi_1^{-1} \\theta_j - 1}}{B(n_{1, j}, \\varphi_0^{-1}p_j) B(n_{2, j}, \\varphi_0^{-1}p_j)} \\mathrm{d} \\, p \\right)^{-1} \\, .$$\n",
    "\n",
    "These integrals are complicated and probably not easy to calculate even with a computer. If odeint isn't able to perform well with this expression, it could be considered to compute the integrals by a Monte Carlo method and use importance sampling to make it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede09ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
